# Geometric Structure LLM: Hyperbolic Manifold Representations

A high-fidelity simulation of Geometric Deep Learning concepts applied to Language Model (LLM) structures. This project visualizes how semantic hierarchies and attention-based relationships are more naturally represented in non-Euclidean space—specifically the Poincaré Ball.

Examines geometric properties of token and attention representations in large language models through three complementary perspectives:

- Hyperbolic embedding of semantic hierarchies  
- Curvature-aware graph structure induced by distances  
- Optimal transport alignment of distributed representations  

All computations are performed in the Poincaré ball model:

- Embeds a semantic hierarchy into hyperbolic space  
- Constructs a distance-induced graph with curvature proxy  
- Aligns perturbed copies of the embeddings via Wasserstein barycenter  
